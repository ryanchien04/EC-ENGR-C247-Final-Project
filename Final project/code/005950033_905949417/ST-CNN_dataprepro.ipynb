{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, ConcatDataset\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n"
     ]
    }
   ],
   "source": [
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "device = torch.device(device)\n",
    "print('Using {} device'.format(device))\n",
    "#device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_test = np.load(\"X_test.npy\")\n",
    "# X_test = scipy.stats.zscore(X_test, axis=1)\n",
    "y_test = np.load(\"y_test.npy\")\n",
    "person_train_valid = np.load(\"person_train_valid.npy\")\n",
    "X_train_valid = np.load(\"X_train_valid.npy\")\n",
    "# X_train_valid = scipy.stats.zscore(X_train_valid, axis=1)\n",
    "y_train_valid = np.load(\"y_train_valid.npy\")\n",
    "person_test = np.load(\"person_test.npy\")\n",
    "y_test -= 769\n",
    "y_train_valid -= 769"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training/Valid data shape: (2115, 22, 1000)\n",
      "Test data shape: (443, 22, 1000)\n",
      "Training/Valid target shape: (2115,)\n",
      "Test target shape: (443,)\n",
      "Person train/valid shape: (2115, 1)\n",
      "Person test shape: (443, 1)\n"
     ]
    }
   ],
   "source": [
    "print ('Training/Valid data shape: {}'.format(X_train_valid.shape))\n",
    "print ('Test data shape: {}'.format(X_test.shape))\n",
    "print ('Training/Valid target shape: {}'.format(y_train_valid.shape))\n",
    "print ('Test target shape: {}'.format(y_test.shape))\n",
    "print ('Person train/valid shape: {}'.format(person_train_valid.shape))\n",
    "print ('Person test shape: {}'.format(person_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx_val = np.random.choice(2115, 423, replace=False)\n",
    "# idx_tra = np.array(list(set(range(2115)).difference(set(idx_val))))\n",
    "# X_train, X_val = X_train_valid[idx_tra], X_train_valid[idx_val] \n",
    "# y_train, y_val = y_train_valid[idx_tra], y_train_valid[idx_val]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing by TA's approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of testing set: (1772, 22, 250)\n",
      "Shape of testing labels: (1772,)\n",
      "Shape of training set: (6960, 22, 250)\n",
      "Shape of validation set: (1500, 22, 250)\n",
      "Shape of training labels: (6960,)\n",
      "Shape of validation labels: (1500,)\n"
     ]
    }
   ],
   "source": [
    "def data_prep(X,y,sub_sample,average,noise):\n",
    "    total_X = None\n",
    "    total_y = None\n",
    "    # Trimming the data (sample,22,1000) -> (sample,22,500)\n",
    "    X = X[:,:,0:500]\n",
    "    # Maxpooling the data (sample,22,1000) -> (sample,22,500/sub_sample)\n",
    "    X_max = np.max(X.reshape(X.shape[0], X.shape[1], -1, sub_sample), axis=3)\n",
    "    total_X = X_max\n",
    "    total_y = y\n",
    "    # Averaging + noise \n",
    "    X_average = np.mean(X.reshape(X.shape[0], X.shape[1], -1, average),axis=3)\n",
    "    X_average = X_average + np.random.normal(0.0, 0.5, X_average.shape)\n",
    "    total_X = np.vstack((total_X, X_average))\n",
    "    total_y = np.hstack((total_y, y))    \n",
    "    # Subsampling\n",
    "    for i in range(sub_sample):\n",
    "        X_subsample = X[:, :, i::sub_sample] + \\\n",
    "                            (np.random.normal(0.0, 0.5, X[:, :,i::sub_sample].shape) if noise else 0.0)\n",
    "        total_X = np.vstack((total_X, X_subsample))\n",
    "        total_y = np.hstack((total_y, y))\n",
    "    return total_X,total_y\n",
    "\n",
    "\n",
    "X_train_valid_prep,y_train_valid_prep = data_prep(X_train_valid,y_train_valid,2,2,True)\n",
    "\n",
    "## Random splitting and reshaping the data\n",
    "# First generating the training and validation indices using random splitting\n",
    "\n",
    "ind_valid = np.random.choice(2115, 375, replace=False)\n",
    "ind_train = np.array(list(set(range(2115)).difference(set(ind_valid))))\n",
    "\n",
    "# Creating the training and validation sets using the generated indices\n",
    "(X_train, X_valid) = X_train_valid[ind_train], X_train_valid[ind_valid] \n",
    "(y_train, y_valid) = y_train_valid[ind_train], y_train_valid[ind_valid]\n",
    "\n",
    "\n",
    "## Preprocessing the dataset\n",
    "X_train,y_train = data_prep(X_train,y_train,2,2,True)\n",
    "X_val,y_val = data_prep(X_valid,y_valid,2,2,True)\n",
    "X_test_prep,y_test_prep = data_prep(X_test,y_test,2,2,True)\n",
    "\n",
    "\n",
    "print('Shape of testing set:',X_test_prep.shape)\n",
    "print('Shape of testing labels:',y_test_prep.shape)\n",
    "\n",
    "print('Shape of training set:',X_train.shape)\n",
    "print('Shape of validation set:',X_val.shape)\n",
    "print('Shape of training labels:',y_train.shape)\n",
    "print('Shape of validation labels:',y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To torch.tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.from_numpy(X_train).float()\n",
    "X_train = X_train[:,None,:,:]\n",
    "y_train = torch.from_numpy(y_train).long()\n",
    "X_val = torch.from_numpy(X_val).float()\n",
    "X_val = X_val[:,None,:,:]\n",
    "y_val = torch.from_numpy(y_val).long()\n",
    "X_test = torch.from_numpy(X_test_prep).float()\n",
    "X_test = X_test[:,None,:,:]\n",
    "y_test = torch.from_numpy(y_test_prep).long()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "val_dataset = torch.utils.data.TensorDataset(X_val, y_val)\n",
    "test_dataset = torch.utils.data.TensorDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #nn.Conv2d(in_channels, out_channels, kernel_size)\n",
    "        #input:(C,H,W)=(1,22,1000)\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 32, (1,64), padding='same') #(32,22,250)\n",
    "        self.bn1   = nn.BatchNorm2d(32) #(32,22,250)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(32, 64, (22,1)) #(64,1,250)\n",
    "        self.bn2   = nn.BatchNorm2d(64) #(64,1,250) \n",
    "        #--->ELU()\n",
    "        self.pool1 = nn.AvgPool2d([1, 5], stride=[1, 5], padding=0) #(64,1,50)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(64, 64, (1,16), padding='same') #(64,1,50)\n",
    "        self.conv4 = nn.Conv2d(64, 64, (1,1), padding='same') #(64,1,50)\n",
    "        self.bn3   = nn.BatchNorm2d(64) #(64,1,50)\n",
    "        #--->ELU()\n",
    "        self.pool2 = nn.AvgPool2d([1, 5], stride=[1, 5], padding=0) #(64,1,10)\n",
    "        \n",
    "        self.fc1 = nn.Linear(64*1*10,256)\n",
    "        self.fc2 = nn.Linear(256,4)\n",
    "        \n",
    "        self.sm = nn.Softmax(dim=1)\n",
    "        \n",
    "        #non-linear\n",
    "        self.nonlinear = nn.ELU(True)\n",
    "        #Dropout\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.bn1(self.conv1(x))\n",
    "        \n",
    "        x = self.nonlinear(self.bn2(self.conv2(x)))\n",
    "        x = self.pool1(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.nonlinear(self.bn3(self.conv4(self.conv3(x))))\n",
    "        x = self.pool2(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        # x = self.sm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, optimizer, criterion, dataloader):\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    model.train()\n",
    "    for i, (inp, labels) in enumerate(dataloader):\n",
    "        inp, labels = inp.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outp = model(inp)\n",
    "        outp_pred = torch.max(outp,axis=1).indices\n",
    "        train_acc += torch.sum(outp_pred == labels)\n",
    "        loss = criterion(outp, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    train_loss = train_loss / len(train_dataloader.dataset)\n",
    "    train_acc = train_acc/ len(train_dataloader.dataset)\n",
    "    return train_loss, train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, optimizer, criterion, dataloader, pred_loader=test_dataloader):\n",
    "    test_loss = 0\n",
    "    test_acc = 0\n",
    "    best_acc = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (inp, labels) in enumerate(pred_loader):\n",
    "            inp, labels = inp.to(device), labels.to(device)\n",
    "            outp = model(inp)\n",
    "            outp_pred = torch.max(outp,axis=1).indices\n",
    "            test_acc += torch.sum(outp_pred == labels)\n",
    "            loss = criterion(outp, labels)\n",
    "            test_loss += loss.item()\n",
    "        test_loss = test_loss / len(pred_loader.dataset)\n",
    "        test_acc = test_acc / len(pred_loader.dataset)\n",
    "    return test_loss, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(num_epochs, optimizer, criterion, pred_loader=test_dataloader):\n",
    "    train_loss_list = []\n",
    "    test_loss_list = []\n",
    "    best_acc = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss, train_acc = train_epoch(model, optimizer, criterion, train_dataloader)\n",
    "        test_loss, test_acc = predict(model, optimizer, criterion, pred_loader)\n",
    "        # if epoch % 5 == 0:\n",
    "        #     print(f'Epoch [{epoch+1}], train_Loss : {train_loss:.4f}, val_Loss : {test_loss:.4f},\\\n",
    "        #         train_accuracy : {100*train_acc:.4f}%, val_accuracy : {100*test_acc:.4f}%')\n",
    "        print(f'Epoch [{epoch+1}], train_Loss : {train_loss:.4f}, val_Loss : {test_loss:.4f},\\\n",
    "                train_accuracy : {100*train_acc:.4f}%, val_accuracy : {100*test_acc:.4f}%')\n",
    "        train_loss_list.append(train_loss)\n",
    "        test_loss_list.append(test_loss)\n",
    "\n",
    "        if test_acc > best_acc and pred_loader != test_dataloader:\n",
    "            torch.save(model.state_dict(), 'CNN_model_TAaug.pt')\n",
    "            best_acc = test_acc\n",
    "    return train_loss_list, test_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_loss_list, test_loss_list \u001b[38;5;241m=\u001b[39m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[48], line 6\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(num_epochs, optimizer, criterion, pred_loader)\u001b[0m\n\u001b[1;32m      4\u001b[0m best_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m----> 6\u001b[0m     train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     test_loss, test_acc \u001b[38;5;241m=\u001b[39m predict(model, optimizer, criterion, pred_loader)\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# if epoch % 5 == 0:\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m#     print(f'Epoch [{epoch+1}], train_Loss : {train_loss:.4f}, val_Loss : {test_loss:.4f},\\\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m#         train_accuracy : {100*train_acc:.4f}%, val_accuracy : {100*test_acc:.4f}%')\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[46], line 14\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, optimizer, criterion, dataloader)\u001b[0m\n\u001b[1;32m     12\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     13\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 14\u001b[0m     train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m train_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_dataloader\u001b[38;5;241m.\u001b[39mdataset)\n\u001b[1;32m     16\u001b[0m train_acc \u001b[38;5;241m=\u001b[39m train_acc\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_dataloader\u001b[38;5;241m.\u001b[39mdataset)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_loss_list, test_loss_list = run(200, optimizer, criterion, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN().to(device)\n",
    "model.load_state_dict(torch.load('CNN_model_TAaug.pt'))\n",
    "model.eval()\n",
    "test_loss, test_acc = predict(model, optimizer, criterion, test_dataloader)\n",
    "\n",
    "print(f'Test_Loss : {test_loss:.4f},Test_Accuracy : {100*test_acc:.4f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 200\n",
    "fig, ax = plt.subplots(figsize=(8, 6), dpi=100)\n",
    "ax.plot(range(num_epochs), train_loss_list, c='b', label='train loss')\n",
    "ax.plot(range(num_epochs), test_loss_list, c='r', label='val loss')\n",
    "ax.set_xlabel('epoch', fontsize='20')\n",
    "ax.set_ylabel('loss', fontsize='20')\n",
    "ax.set_title('train and val loss', fontsize='20')\n",
    "ax.grid()\n",
    "ax.legend(fontsize='25')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
